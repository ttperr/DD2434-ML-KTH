\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{fancyhdr} % Header
\usepackage{lastpage}
\usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{float} % Floating position
\usepackage{hyperref} % Links
\usepackage{amsmath} % Math
\usepackage{amssymb} % Math
\usepackage{pdfpages} % Import pdf

\graphicspath{{images/}}

\newcommand{\authorFst}{Tristan Perrot}
\newcommand{\emailFst}{\href{mailto:tristanp@kth.se}{tristanp@kth.se}}
\newcommand{\authorSnd}{Étienne Riguet}
\newcommand{\emailSnd}{\href{mailto:riguet@kth.se}{riguet@kth.se}}

\pagestyle{fancy}
\fancyhf{} % clear all header and footer fields
\lhead{Assignment 2A \\ DD2434 - Machine Learning, Advanced Course}
\rhead{\authorFst \\ \authorSnd}
\cfoot{\thepage \  / \pageref{LastPage}}
\setlength{\headheight}{23pt}
\setlength{\footskip}{70pt}

\title{DD2434 - Machine Learning, Advanced Course \\ Assignment 2A}
\author{\authorFst \\ \emailFst \and \authorSnd \\ \emailSnd}
\date{December 2023}

\begin{document}

\maketitle

\begin{center}
    \includegraphics[scale=0.5]{KTH_logo_RGB_bla.png}
\end{center}

\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

\section{Principal Component Analysis}

\subsection{Question 1}

Centering the data is a crucial step in Principal Component Analysis (PCA) because it removes the mean bias and makes the interpretation of principal components more straightforward. Indeed, if the data is not centered, the first principal component will be the direction of the mean of the data, and the second principal component will be the direction of the variance of the data. Therefore, the first principal component will not be the direction of the maximum variance of the data, and the second principal component will not be the direction of the second maximum variance of the data. In other words, the principal components will not be the directions of the maximum variance of the data.

\subsection{Question 2}

TODO

\subsection{Question 3}

The \textit{variance} of the dataset $\mathcal{Y}$ is defined as $\text{Var}(\mathcal{Y}) = \sum_{y \in \mathcal{Y}} \left\lVert y - \overline{y} \right\rVert^2_2 = \sum_{y \in \mathcal{Y}} \left\lVert y \right\rVert^2_2$ here because the data is centered. Therefore we have:
\begin{equation}
    \begin{split}
        \text{Var}(\mathcal{Y}) = \sum_{y \in \mathcal{Y}} \left\lVert y \right\rVert^2_2 &= \sum_{y \in \mathcal{Y}} y^T y \\
        &= \sum_{i = 1}^d (Y^T Y)_{i, i} \\
        &= \text{Tr}(Y^T Y) \\
        &= \text{Tr}(V \Sigma^T U^T U \Sigma V^T) \\
        &= \text{Tr}(V \Sigma^T \Sigma V^T) \\
        &= \text{Tr}(\Sigma^T \Sigma) \\
        \text{Var}(\mathcal{Y}) &= \sum_{i = 1}^d \sigma_i^2
    \end{split}
\end{equation}

\subsection{Question 4}

The \textit{variance} of the projected dataset after PCA is $Var(\mathcal{X}) = \sum_{x \in \mathcal{X}} \left\lVert x \right\rVert^2_2$. Where $X = W^TY$ is a $k \times n$ projected matrix. Therefore we have:
\begin{equation}
    \begin{split}
        \text{Var}(\mathcal{X}) = \sum_{x \in \mathcal{X}} \left\lVert x \right\rVert^2_2 &= \sum_{x \in \mathcal{X}} x^T x \\
        &= \sum_{i = 1}^k (X^T X)_{i, i} \\
        &= \text{Tr}(X^T X) \\
        &= \text{Tr}(Y^T W W^T Y) \\
        &= \text{Tr}(W W^T Y Y^T) \\
        &= \text{Tr}(W W^T V \Sigma^T \Sigma V^T) \\
        &= \text{Tr}(W W^T \Sigma^T \Sigma) \\
        \text{Var}(\mathcal{Y}) &= \sum_{i = 1}^k \sigma_i^2
    \end{split}
\end{equation}

\subsection{Question 5}
The residual data points are $\mathcal{Z} = {z_1, ..., z_n}$ where $z_i = y_i - W W^T y_i$. Therefore we have:
\begin{equation}
    \begin{split}
        \text{Var}(\mathcal{Z}) = \sum_{z \in \mathcal{Z}} \left\lVert z \right\rVert^2_2 &= \sum_{z \in \mathcal{Z}} z^T z \\
        &= \sum_{y \in \mathcal{Y}} (y - W W^T y)^T (y - W W^T y) \\
        &= \sum_{y \in \mathcal{Y}} y^T y - y^T W W^T y - (W W^T y)^T y + (W W^T y)^T W W^T y \\
        &= \sum_{y \in \mathcal{Y}} y^T y - y^T W W^T y - y^T W W^T y + y^T W W^T W W^T y \\
        &= \sum_{y \in \mathcal{Y}} y^T y - 2 y^T W W^T y + y^T W W^T y \\
        &= \sum_{y \in \mathcal{Y}} y^T y - y^T W W^T y \\
        &= \sum_{i = 1}^d \sigma_i^2 - \sum_{i = 1}^k \sigma_i^2 \\
        \text{Var}(\mathcal{Z}) &= \sum_{i = k + 1}^d \sigma_i^2
    \end{split}
\end{equation}
And therefore we conclude that variance of original data = variance explained by PCA + variance of residual data.

\section{PCA vs. Johnson-Lindenstrauss random projections}

TODO

\section{Programming task — MDS}

TODO

\newpage
\appendix
\section{Appendix}

\end{document}