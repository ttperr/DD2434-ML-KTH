\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{fancyhdr} % Header
\usepackage{lastpage}
\usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{float} % Floating position
\usepackage{hyperref} % Links
\usepackage{amsmath} % Math
\usepackage{amssymb} % Math
\usepackage{bbold} % Math (for indicatrice function)
\usepackage{pdfpages} % Include PDF

\graphicspath{{images/}}

\newcommand{\authorFst}{Tristan Perrot}
\newcommand{\emailFst}{\href{mailto:tristanp@kth.se}{tristanp@kth.se}}

\pagestyle{fancy}
\fancyhf{} % clear all header and footer fields
\lhead{Assignment 2B \\ DD2434 - Machine Learning, Advanced Course}
\rhead{\authorFst}
\cfoot{\thepage \  / \pageref{LastPage}}
\setlength{\headheight}{23pt}
\setlength{\footskip}{70pt}

\title{DD2434 - Machine Learning, Advanced Course \\ Assignment 2B}
\author{\authorFst \\ \emailFst}
\date{December 2023}

\begin{document}

\maketitle

\begin{center}
  \includegraphics[scale=0.5]{KTH_logo_RGB_bla.png}
\end{center}

\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

\section{Multidimensional Scaling (MDS) and Isomap}

\subsection{Question 1}
The intuitive reason that the "double centering" trick works is that, as for the PCA, we want to center and then here we want to center by subtracting the overall mean and therefore the mean for the columns and the rows. This is why subtracting the mean twice works.

\subsection{Question 2}
While the double centering method center the data around the origin, the "first point" trick center the data around the first point of the dataset. Therefore the solution will be different but in MDS we are only interested in the relative position of the points and not their absolute position. Therefore the solution will be the same up to a translation.

\subsection{Question 3}
As stated, the classical MDS algorithm when $Y$ is known is based on the eigen-decomposition of $S = Y^T Y$. Then, the singular values of $Y$ are the nonnegative square roots of the eigenvalues of $S$.
PCA on $Y$ is based on the singular value decomposition of $Y = U \Sigma V^T$. Then, the singular values of $Y$ are the diagonal entries of $\Sigma$. Therefore the methods are equivalent because they both calculate the singular values of $Y$ to reduce the dimension by getting the directions with the highest variance.
When $n$ is much larger than $p$ (many observations, few dimensions) PCA will tend to be more efficient because it avoids the need to calculate and store a large distance matrix.

\subsection{Question 4}
During the Isomap method, the process to obtain the neighborhood graph may yield a disconnected graph. For example, if we have two clusters of points that are well separated and a small $p$, each point will be connected to there $p$ nearest neighbors and therefore the graph will be disconnected.

\subsection{Question 5}
Imagine as stated above a dataset with two clusters well separated that yield a disconnected graph. Now, we could search the two closest points in the separated clusters and connect them by an weighted edge of there distance in the dataset. We could repeat this if there is more than 2 clusters until the graph is connected.
This method is based on the fact that we need to have a fully connected graph but we still want to well describe the distance between the points. Therefore, it is expected to work well in practice.

\section{Success probability in the Johnson-Lindenstrauss lemma}

\subsection{Question 6}
Let us denote the probability of success of an trial $p$ and the probability of failure $q = 1 - p$ and the number of trials $n$. We know that $p \geq \frac{1}{n}$. We want to have $q^k \leq 0.05$. Which means:
\begin{align*}
  q^k     \leq 0.05 & \Leftrightarrow \ln q^k \leq \ln 0.05                         \\
                    & \Leftrightarrow k \geq \frac{\ln 0.05}{\ln q}                 \\
                    & \Leftrightarrow k \geq \frac{\ln 0.05}{\ln (1 - p)}           \\
                    & \Leftrightarrow k \geq \frac{\ln 0.05}{\ln (1 - \frac{1}{n})}
\end{align*}
With a high $n$ we have $\ln (1 - \frac{1}{n}) \approx -\frac{1}{n}$ and therefore:
\begin{align*}
  k \geq \frac{\ln 0.05}{\ln (1 - \frac{1}{n})} & \Leftrightarrow k \geq \frac{\ln 0.05}{-\frac{1}{n}} \\
                                                & \Leftrightarrow k \geq -n \ln 0.05
\end{align*}
Therefore, a $\mathcal{O}(n)$ independent trials are sufficient to ensure that the probability of success is at least $95\%$.

\section{Node similarity for representation learning}

\subsection{Question 7}
The matrix \( P = D^{-1}A \) can be thought of as a transition probability matrix in a random walk on the graph, where \( D \) is the degree matrix and \( A \) is the adjacency matrix. The element \( P_{ij} \) gives the probability of moving from node \( i \) to node \( j \) in a single step of the random walk.
Therefore, the element \( (P^k)_{ij} \) gives the probability of reaching node \( j \) from node \( i \) in exactly \( k \) steps.
And then, the factor \( \alpha^k \) discounts the influence of longer paths in the similarity measure. Since \( 0 < \alpha < 1 \), the longer the path (i.e., the larger the value of \( k \)), the less it contributes to the overall similarity.
By summing over all powers of \( k \), the definition of \( S_{ij} \) considers paths of all lengths, but with diminishing weights for longer paths. This infinite series converges because \( \alpha < 1 \) and \( \|P\| \leq 1 \), given that \( P \) is a probability matrix.
The resulting similarity measure \( S_{ij} \) captures not just the direct connections (as given by the adjacency matrix \( A \)) but also the global structure of the graph by incorporating the effect of paths of all lengths.

\subsection{Question 8}
TODO

\section{Spectral graph analysis}

\subsection{Question 9}
TODO

\subsection{Question 10}
TODO

\subsection{Question 11}
TODO

\section{Programming task}

\subsection{Question 12}
TODO

\newpage
\appendix

\section{Appendix}


\end{document}