\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{fancyhdr} % Header
\usepackage{lastpage}
\usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{float} % Floating position
\usepackage{hyperref} % Links
\usepackage{amsmath} % Math
\usepackage{amssymb} % Math
\usepackage{bbold} % Math (for indicatrice function)
\usepackage{pdfpages} % Include PDF

\graphicspath{{images/}}

\newcommand{\authorFst}{Tristan Perrot}
\newcommand{\emailFst}{\href{mailto:tristanp@kth.se}{tristanp@kth.se}}

\pagestyle{fancy}
\fancyhf{} % clear all header and footer fields
\lhead{Assignment 2B \\ DD2434 - Machine Learning, Advanced Course}
\rhead{\authorFst}
\cfoot{\thepage \  / \pageref{LastPage}}
\setlength{\headheight}{23pt}
\setlength{\footskip}{70pt}

\title{DD2434 - Machine Learning, Advanced Course \\ Assignment 2B}
\author{\authorFst \\ \emailFst}
\date{December 2023}

\begin{document}

\maketitle

\begin{center}
  \includegraphics[scale=0.5]{KTH_logo_RGB_bla.png}
\end{center}

\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

\section{Multidimensional Scaling (MDS) and Isomap}

\subsection{Question 1}
The intuitive reason that the "double centering" trick works is that, as for the PCA, we want to center and then here we want to center by subtracting the overall mean and therefore the mean for the columns and the rows. This is why subtracting the mean twice works.

\subsection{Question 2}
While the double centering method center the data around the origin, the "first point" trick center the data around the first point of the dataset. Therefore the solution will be different but in MDS we are only interested in the relative position of the points and not their absolute position. Therefore the solution will be the same up to a translation.

\subsection{Question 3}
As stated, the classical MDS algorithm when $Y$ is known is based on the eigen-decomposition of $S = Y^T Y$. Then, the singular values of $Y$ are the nonnegative square roots of the eigenvalues of $S$.
PCA on $Y$ is based on the singular value decomposition of $Y = U \Sigma V^T$. Then, the singular values of $Y$ are the diagonal entries of $\Sigma$. Therefore the methods are equivalent because they both calculate the singular values of $Y$ to reduce the dimension by getting the directions with the highest variance.
When $n$ is much larger than $p$ (many observations, few dimensions) PCA will tend to be more efficient because it avoids the need to calculate and store a large distance matrix.

\subsection{Question 4}
During the Isomap method, the process to obtain the neighborhood graph may yield a disconnected graph. For example, if we have two clusters of points that are well separated and a small $p$, each point will be connected to there $p$ nearest neighbors and therefore the graph will be disconnected.

\subsection{Question 5}
TODO

\section{Success probability in the Johnson-Lindenstrauss lemma}

\subsection{Question 6}
TODO

\section{Node similarity for representation learning}

\subsection{Question 7}
TODO

\subsection{Question 8}
TODO

\section{Spectral graph analysis}

\subsection{Question 9}
TODO

\subsection{Question 10}
TODO

\subsection{Question 11}
TODO

\section{Programming task}

\subsection{Question 12}
TODO

\newpage
\appendix

\section{Appendix}


\end{document}